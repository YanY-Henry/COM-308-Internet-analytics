{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *Your group letter.*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *TAY Ying Xu Dempster*\n",
    "* *LIU Linqi*\n",
    "* *YAN Yuhang*\n",
    "* *SONG Yifei*\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'courseId': 'MSE-440', 'name': 'Composites technology', 'description': \"The latest developments in processing and the novel generations of organic composites are discussed. Nanocomposites, adaptive composites and biocomposites are presented. Product development, cost analysis and study of new markets are practiced in team work. Content Basics of composite materialsConstituentsProcessing of compositesDesign of composite structures\\xa0Current developmentNanocomposites Textile compositesBiocompositesAdaptive composites\\xa0ApplicationsDriving forces and marketsCost analysisAerospaceAutomotiveSport Keywords Composites - Applications - Nanocomposites - Biocomposites - Adaptive composites - Design - Cost Learning Prerequisites Required courses Notion of polymers Recommended courses Polymer Composites Learning Outcomes By the end of the course, the student must be able to: Propose suitable design, production and performance criteria for the production of a composite partApply the basic equations for process and mechanical properties modelling for composite materialsDiscuss the main types of composite applications Transversal skills Use a work methodology appropriate to the task.Use both general and domain specific IT resources and toolsCommunicate effectively with professionals from other disciplines.Evaluate one's own performance in the team, receive and respond appropriately to feedback. Teaching methods Ex cathedra and invited speakers Group sessions with exercises or work on the project Expected student activities Attendance at lectures Design of a composite part, bibliography search \\xa0 Assessment methods Written exam report and oral presentation in class\"}\n"
     ]
    }
   ],
   "source": [
    "print(courses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split connected words (eg. materialsConstituentsProcessing but not for acronyms like IT)\n",
    "\n",
    "for course in courses:\n",
    "\twords = []\n",
    "\tcurrent_word = ''\n",
    "\n",
    "\tfor word in course['description'].split():\n",
    "\t\tfor char in word:\n",
    "\t\t\t# If char is uppercase, it is the start of a new word\n",
    "\t\t\tif char.isupper():\n",
    "\t\t\t\t# Don't add word yet if it is all uppercase (acronym)\n",
    "\t\t\t\tif current_word.isupper() and len(current_word) > 0:\n",
    "\t\t\t\t\tcurrent_word += char\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t# Add current word\n",
    "\t\t\t\twords.append(current_word)\n",
    "\t\t\t\tcurrent_word = char\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_word += char\n",
    "\n",
    "\t\twords.append(current_word)\n",
    "\t\tcurrent_word = ''\n",
    "\n",
    "\tcourse['description'] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "for course in courses:\n",
    "\tcourse['description'] = ''.join([char for char in course['description'] if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and capitalized versions\n",
    "\n",
    "stopwords_capitalized = [s.capitalize() for s in stopwords]\n",
    "\n",
    "for course in courses:\n",
    "\tcourse['description'] = ' '.join([w for w in course['description'].split() if w not in stopwords and w not in stopwords_capitalized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check word frequency\n",
    "\n",
    "freq_dict = {}\n",
    "\n",
    "for course in courses:\n",
    "  for word in course['description'].split():\n",
    "    if word in freq_dict:\n",
    "      freq_dict[word] += 1\n",
    "    else:\n",
    "      freq_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('methods', 1592), ('Learning', 1239), ('student', 1177), ('Content', 835), ('courses', 757), ('systems', 697), ('end', 661), ('students', 655), ('design', 603), ('Outcomes', 600)]\n",
      "[('biocomposites', 1), ('Constituents', 1), ('Textile', 1), ('Automotive', 1), ('Sport', 1), ('photograph', 1), ('blot', 1), ('extracted', 1), ('reproducibility', 1), ('pipelines', 1)]\n"
     ]
    }
   ],
   "source": [
    "frequent_sorted = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "infrequent_sorted = sorted(freq_dict.items(), key=lambda x: x[1])\n",
    "print(frequent_sorted[:10])\n",
    "print(infrequent_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose infrequent words that only appear once\n",
    "\n",
    "infrequent_sorted = [w for w in infrequent_sorted if w[1] == 1]\n",
    "len(infrequent_sorted)\n",
    "\n",
    "# Make lists of frequent and infrequent words\n",
    "\n",
    "frequent_list = [w[0] for w in frequent_sorted[:10]]\n",
    "infrequent_list = [w[0] for w in infrequent_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest developments processing generations organic composites discussed Nanocomposites adaptive composites presented Product development cost analysis study markets practiced team work Basics composite materials Processing composites Design composite structures Current development Nanocomposites composites Biocomposites Adaptive composites Applications Driving forces markets Cost analysis Aerospace Keywords Composites Applications Nanocomposites Biocomposites Adaptive composites Design Cost Prerequisites Required Notion polymers Recommended Polymer Composites Propose suitable production performance criteria production composite part Apply basic equations process mechanical properties modelling composite materials Discuss main types composite applications Transversal skills work methodology task general domain specific IT resources tools Communicate effectively professionals disciplines Evaluate performance team receive respond appropriately feedback Teaching cathedra invited speakers Group sessions exercises work project Expected activities Attendance lectures Design composite part bibliography search Assessment Written exam report oral presentation class\n"
     ]
    }
   ],
   "source": [
    "# Remove frequent and infrequent words\n",
    "\n",
    "for course in courses:\n",
    "\tcourse['description'] = ' '.join([w for w in course['description'].split() if w not in frequent_list and w not in infrequent_list])\n",
    "\n",
    "print(courses[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We don't tranform all the words to lowercase, and we don't perform stemming\n",
    "or lemmatization compared to pre-processing in Exercise 4.1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we try to find the most frequent word that is in `courses['description']` and the word list of Word2Vec model. The most frequent word will work as a default word for the words in `courses['description']` and not present in the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_frequent_word_in_description(words, model_path):\n",
    "    # Filter out words not present in the Word2Vec model\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\n",
    "    word_counts = Counter(valid_words)\n",
    "\n",
    "    # Find the most frequent word\n",
    "    if word_counts:\n",
    "        most_frequent_word = word_counts.most_common(1)[0][0]\n",
    "        return most_frequent_word\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/ix/model.txt')\n",
    "all_words = []\n",
    "for course in courses:\n",
    "    all_words.extend(course['description'].split())\n",
    "most_frequent_word = most_frequent_word_in_description(all_words, model)\n",
    "print(most_frequent_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If it cannot find a word in the model, it will return the vector of the default word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return model[most_frequent_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = {word: get_vector(word) for word in all_words}\n",
    "word_vectors = dict(map(lambda word: (word, get_vector(word)), all_words))\n",
    "\n",
    "# Normalize the word vectors\n",
    "vectors = np.array(list(word_vectors.values()))\n",
    "normalized_vectors = normalize(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform KMeans algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0, init='k-means++')\n",
    "kmeans = kmeans.fit(normalized_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the top-10 words in each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: structures, adjacent, areas, connecting, encompassing, stretching, connected, surrounding, area, complex\n",
      "Cluster 1: neuronal, metabolic, vitro, biochemical, hematopoietic, metabolism, angiogenesis, inhibition, bacterial, tissues\n",
      "Cluster 2: Nanocomposites, analysis, Biocomposites, Prerequisites, Propose, IT, timelapse, FIJI, Illustrate, Integrate\n",
      "Cluster 3: Implementation, Analysis, Evaluation, Methodology, Policies, Assessment, Context, Policy, Strategy, Integration\n",
      "Cluster 4: thin, vertical, horizontal, mesh, surfaces, stacked, cylindrical, metallic, stretchable, thick\n",
      "Cluster 5: Baker, Smith, Robinson, Taylor, Freeman, Patterson, Meyer, Cummings, Evans, Rosenthal\n",
      "Cluster 6: seminars, lectures, presentations, teaching, seminar, lecturers, tutorials, symposia, instructors, professors\n",
      "Cluster 7: software, interfaces, implementations, Simulink, functionality, APIs, interface, opensource, debugging, scalable\n",
      "Cluster 8: quadratic, ODEs, equations, discretized, finite, summability, stationarity, eigenvalues, univariate, generalized\n",
      "Cluster 9: polymers, molecules, dopants, nanoparticles, macromolecules, ions, functionalization, azides, adsorbent, compounds\n",
      "Cluster 10: optimal, utilization, optimum, mechanisms, robustness, processes, flexibility, variability, correlated, affect\n",
      "Cluster 11: Mind, Make, Shape, Perfect, Noise, Memories, Hands, Write, Understand, Explain\n",
      "Cluster 12: 15, 14, 13, 22, 16, 21, 26, 27, 23, 29\n",
      "Cluster 13: inform, knowing, informed, concerned, intervene, realize, aware, find, bring, hope\n",
      "Cluster 14: Biochemistry, Microbiology, Physics, Neuroscience, Biophysics, Chemistry, Biomedical, Computational, Pharmacology, Applied\n",
      "Cluster 15: investments, investment, enterprises, investing, enterprise, pricing, institutional, outsourcing, financial, incentives\n",
      "Cluster 16: Optimization, Measurement, Applications, Multiscale, Resistive, Perturbation, Kinematics, Iterative, Processes, Imaging\n",
      "Cluster 17: photodetectors, nonlinearities, optical, reflectometry, electromagnetic, excitation, linewidth, waveguides, dielectric, tunable\n",
      "Cluster 18: groundwater, biomass, evaporation, wastewaters, water, pollutants, sedimentation, vadose, soils, seepage\n",
      "Cluster 19: concepts, context, notion, notions, understanding, interpretation, emphasizing, perspectives, perspective, normative\n"
     ]
    }
   ],
   "source": [
    "def get_top_words_in_clusters(kmeans, word_vectors, top_n=10):\n",
    "    cluster_centroids = kmeans.cluster_centers_\n",
    "    top_words = {}\n",
    "    \n",
    "    words = list(word_vectors.keys())\n",
    "    vectors = np.array(list(word_vectors.values()))\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    for cluster_idx in range(len(cluster_centroids)):\n",
    "        centroid = cluster_centroids[cluster_idx]\n",
    "        \n",
    "        # Get the indices of word vectors that belong to the current cluster\n",
    "        cluster_word_indices = [i for i, label in enumerate(labels) if label == cluster_idx]\n",
    "        cluster_words = [words[i] for i in cluster_word_indices]\n",
    "        cluster_vectors = [vectors[i] for i in cluster_word_indices]\n",
    "\n",
    "        if cluster_vectors:\n",
    "            similarities = model.cosine_similarities(centroid, cluster_vectors)\n",
    "            word_similarity_pairs = zip(cluster_words, similarities)\n",
    "            \n",
    "            sorted_words = sorted(word_similarity_pairs, key=lambda x: x[1], reverse=True)\n",
    "            top_words[cluster_idx] = [word for word, _ in sorted_words[:top_n]]\n",
    "        else:\n",
    "            top_words[cluster_idx] = []\n",
    "\n",
    "    return top_words\n",
    "\n",
    "top_words = get_top_words_in_clusters(kmeans, word_vectors)\n",
    "for cluster_idx, words in top_words.items():\n",
    "    print(f\"Cluster {cluster_idx}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number of clusters:**\n",
    "\n",
    "We consider 20 to be a good choice of numnber of cluster, because EPFL has 18 sections and we may choose k value close to 18. With k=20, we can preserve the diversity of the clusters, while not having different clusters for similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe the following types of clusters: structure analyses, Biology, Material Science, policies and strategy, education, description and charateristic, finance, names and identities, subjects, mathematics, numbers, verbs, environment sciences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we give labels to the representative cluster of its type**\n",
    "\n",
    "1. Structure Analyses: Cluster 0\n",
    "\n",
    "2. Biology: Cluster 1\n",
    "\n",
    "3. Material Science and Chemistry: Cluster 2\n",
    "\n",
    "4. Names and Identity: Cluster 5\n",
    "\n",
    "5. Education: Cluster 6\n",
    "\n",
    "6. Software development: Cluster 7\n",
    "\n",
    "7. Mathematics: Cluster 8\n",
    "\n",
    "8. Finance and Management: Cluster 15\n",
    "\n",
    "9. Physics: Cluster 17\n",
    "\n",
    "10. Environment Science: Cluster 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compared to LSI and LDA:**\n",
    "\n",
    "**With LSI**:\n",
    "\n",
    "W2V's results are more consistent within the same cluster, while LSI's seems not clustering well. LSI's results contain some topics unseen in W2V's results, such as \"Philosophy\", \"Wireless\", and \"Security\". Still, the two methods get a few similar topics, such as \"Market\" and \"Finance and Management\", \"Chemistry\" and \"Material Science and Chemistry\".\n",
    "\n",
    "**With LDA**:\n",
    "\n",
    "Both of them get topics like \"Biology\", \"Chemistry\", \"Management\"; some topics from the results are not exactly the same but are quite related, such as \"Environment Science\" and \"Urban Development\", \"Mathematics\" and \"Optics and Computation\". Besides, W2V's results contain topic like \"Names and Identity\", which is not seen in both LSI and LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement an aggregation of the `word_vectors` using the weighted average, whose weights are the TF-IDF scores of the corresponding words.\n",
    "Use the vector aggregation to perform a similarity search with cosine similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reuse the model in 4.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the mapping between terms and their indices, and documents and their indices\n",
    "terms_indices = list(set(all_words))\n",
    "\n",
    "document_indices = [x['courseId'] for x in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9324, 854)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct matrix X\n",
    "\n",
    "M = len(terms_indices)\n",
    "N = len(document_indices)\n",
    "\n",
    "X = np.zeros((M, N))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate matrix X\n",
    "\n",
    "for i, word in enumerate(terms_indices):\n",
    "  for j, course in enumerate(courses):\n",
    "    description_list = course['description'].split()\n",
    "    freq = description_list.count(word)\n",
    "    X[i][j] = freq / len(description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.95817172],\n",
       "       [6.05678401],\n",
       "       [3.91671785],\n",
       "       ...,\n",
       "       [6.05678401],\n",
       "       [6.05678401],\n",
       "       [6.05678401]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct IDF\n",
    "\n",
    "idf = np.zeros(len(terms_indices))\n",
    "\n",
    "for row in range(len(terms_indices)):\n",
    "\tfreq = 0\n",
    "\tfor val in X[row]:\n",
    "\t\tif val > 0:\n",
    "\t\t\tfreq += 1\n",
    "\tidf[row] = np.log(len(courses) / freq)\n",
    "\n",
    "idf = idf.reshape(len(idf), 1)\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TF-IDF matrix\n",
    "\n",
    "tf_idf = X * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the weighted average embedding for each document(course).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.48352061e-03,  4.32334171e-04,  2.02237279e-03, ...,\n",
       "        -2.25496688e-03, -6.04416186e-04,  1.57838411e-04],\n",
       "       [ 1.12673652e-03, -2.88368465e-04,  1.68587675e-03, ...,\n",
       "         6.62948005e-04,  5.71249984e-04,  6.30495779e-04],\n",
       "       [-3.49563314e-04, -3.48699032e-05,  8.84284440e-04, ...,\n",
       "         3.41499341e-04,  5.61502064e-04,  6.89147055e-05],\n",
       "       ...,\n",
       "       [ 1.88111444e-04,  4.61974239e-04,  9.05692170e-04, ...,\n",
       "         6.93232461e-04,  2.11196657e-05,  2.76320381e-04],\n",
       "       [ 7.28788972e-03, -1.33930138e-04,  8.71425867e-03, ...,\n",
       "         2.79346434e-03,  1.26116828e-03,  1.17385818e-03],\n",
       "       [ 2.88377260e-03,  1.63334000e-04,  2.61979504e-03, ...,\n",
       "        -3.80084603e-05,  8.05529184e-04,  1.05522841e-03]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_represent = np.zeros((N, 300))\n",
    "\n",
    "for j in range(N):\n",
    "    description_list = courses[j]['description'].split()\n",
    "    weighted_vectors = []\n",
    "    for word in description_list:\n",
    "        word_idx = terms_indices.index(word)\n",
    "        word_vector = word_vectors[word]\n",
    "        # Weighted word vector\n",
    "        word_vector = tf_idf[word_idx, j]*word_vector\n",
    "        weighted_vectors.append(word_vector)\n",
    "    weighted_average = sum(weighted_vectors)/len(weighted_vectors)\n",
    "    document_represent[j, :] = weighted_average\n",
    "document_represent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute vectors representation for two queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov chains\n",
    "vectors = []\n",
    "for word in ['Markov', 'chains']:\n",
    "    word_vector = get_vector(word)\n",
    "    # Perform average without weighting\n",
    "    vectors.append(word_vector)\n",
    "markov_chain_represent = sum(vectors)/len(vectors)\n",
    "\n",
    "# Facebook\n",
    "facebook_represent = get_vector('Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare two documents\n",
    "\n",
    "def cosine_similarity(doc1, doc2):\n",
    "\tnorm_doc1 = np.linalg.norm(doc1)\n",
    "\tnorm_doc2 = np.linalg.norm(doc2)\n",
    "\treturn np.dot(doc1, doc2) / (norm_doc1 * norm_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"Markov chains\"\n",
    "\n",
    "markov_similarity = []\n",
    "for idx in range(len(document_indices)):\n",
    "  markov_similarity.append(cosine_similarity(document_represent[idx], markov_chain_represent))\n",
    "\n",
    "# Dictionary {course name: similarity value}\n",
    "markov_sim_vals = {}\n",
    "for idx, sim in enumerate(markov_similarity):\n",
    "  markov_sim_vals[courses[idx]['name']] = sim\n",
    "\n",
    "# Sort by similarity value\n",
    "markov_sim_vals_sorted = sorted(markov_sim_vals.items(), key=lambda x:x[1], reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"Facebook\"\n",
    "\n",
    "facebook_similarity = []\n",
    "for idx in range(len(document_indices)):\n",
    "    facebook_similarity.append(cosine_similarity(document_represent[idx], facebook_represent))\n",
    "\n",
    "# Dictionary {course name: similarity value}\n",
    "facebook_sim_vals = {}\n",
    "for idx, sim in enumerate(facebook_similarity):\n",
    "    facebook_sim_vals[courses[idx]['name']] = sim\n",
    "\n",
    "# Sort by similarity value\n",
    "facebook_sim_vals_sorted = sorted(facebook_sim_vals.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the top five courses together with their similarity score for each query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov chains - top five courses with similarity score\n",
      "('Applied probability & stochastic processes', 0.7490182486824046)\n",
      "('Applied stochastic processes', 0.7008952611238507)\n",
      "('Markov chains and algorithmic applications', 0.6525220430300869)\n",
      "('Statistical Sequence Processing', 0.6281523846854378)\n",
      "('Mathematical models in supply chain management', 0.5444954972033781)\n",
      "\n",
      "Facebook - top five courses with similarity score\n",
      "('Computational Social Media', 0.5967716881078419)\n",
      "('Human computer interaction', 0.4811589492949727)\n",
      "('Social media', 0.4639274464913632)\n",
      "('Internet analytics', 0.460725455455456)\n",
      "('Image communication', 0.4440662471067638)\n"
     ]
    }
   ],
   "source": [
    "# markov chain\n",
    "print(\"Markov chains - top five courses with similarity score\")\n",
    "for i in range(5):\n",
    "    print(markov_sim_vals_sorted[i])\n",
    "\n",
    "# facebook\n",
    "print(\"\\nFacebook - top five courses with similarity score\")\n",
    "for i in range(5):\n",
    "    print(facebook_sim_vals_sorted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare with what you obtain for the vector space model and LSI:**\n",
    "\n",
    "The top five courses obtained for this vector space model are similar to what we obtained by LSI. The result for searching 'Markov chains' and 'Facebook' seems more reasonable and related compared to the result from LSI. For 'Facebook', LSI only identifies 1 related course 'Computational Social Media', and assigns 0 similarity score for all other courses. There may be two reasons for this: \n",
    "\n",
    "1. we can preserve the uppercases using the word2vec model, and thus preserve the meaning of some case sensitive terms; \n",
    "2. we use the weighted average as a document representation which give us a more accurate representation;\n",
    "3. Word2Vec can handle synonyms better due to its contextual embeddings. For example, it understands that 'Facebook' and 'social media platform' are related. LSI, on the other hand, might treat these terms as separate, so it cannot find anything related in most courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for \"MySpace Orkut\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for word in ['MySpace', 'Orkut']:\n",
    "    word_vector = get_vector(word)\n",
    "    # Perform average without weighting\n",
    "    vectors.append(word_vector)\n",
    "myspace_orkut_represent = sum(vectors)/len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"MySpace Orkut\"\n",
    "\n",
    "myspace_orkut_similarity = []\n",
    "for idx in range(len(document_indices)):\n",
    "  myspace_orkut_similarity.append(cosine_similarity(document_represent[idx], myspace_orkut_represent))\n",
    "\n",
    "# Dictionary {course name: similarity value}\n",
    "myspace_orkut_sim_vals = {}\n",
    "for idx, sim in enumerate(myspace_orkut_similarity):\n",
    "  myspace_orkut_sim_vals[courses[idx]['name']] = sim\n",
    "\n",
    "# Sort by similarity value\n",
    "myspace_sim_vals_sorted = sorted(myspace_orkut_sim_vals.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**display the top five results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySpace Orkut - top five courses with similarity score\n",
      "('Computational Social Media', 0.5576601987460832)\n",
      "('Human computer interaction', 0.4990797492983756)\n",
      "('Social media', 0.48222115789269315)\n",
      "('Internet analytics', 0.47391868515121005)\n",
      "('Image communication', 0.46852260201614826)\n"
     ]
    }
   ],
   "source": [
    "print(\"MySpace Orkut - top five courses with similarity score\")\n",
    "for i in range(5):\n",
    "    print(myspace_sim_vals_sorted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the results with what you obtained for \"Facebook\" in the previous exercise:**\n",
    "\n",
    "The results are nearly identical, except some differences in the similarity score. The results make sense because \"MySpace\" and \"Orkut\" were both early social networking platforms that predated Facebook. They are all the similar type of applications, and it's very likely that they will be mentioned in the same courses that talks about social media and Internet, e.g. Computational Social Media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for \"coronavirus\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronavirus_represent = get_vector('coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"coronavirus\"\n",
    "\n",
    "coronavirus_similarity = []\n",
    "for idx in range(len(document_indices)):\n",
    "  coronavirus_similarity.append(cosine_similarity(document_represent[idx], coronavirus_represent))\n",
    "\n",
    "# Dictionary {course name: similarity value}\n",
    "coronavirus_sim_vals = {}\n",
    "for idx, sim in enumerate(coronavirus_similarity):\n",
    "  coronavirus_sim_vals[courses[idx]['name']] = sim\n",
    "\n",
    "# Sort by similarity value\n",
    "coronavirus_sim_vals_sorted = sorted(coronavirus_sim_vals.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display top 5 results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus - top five courses with similarity score\n",
      "('Infection biology', 0.5655043101774891)\n",
      "('Biotechnology lab (for CGC)', 0.5200209800804626)\n",
      "('Practical - Blokesch Lab', 0.5182632365497325)\n",
      "('Practical - Lemaitre Lab', 0.5111887713254325)\n",
      "('Neurosciences I : molecular neuroscience and neurodegeneration', 0.5021366636642971)\n"
     ]
    }
   ],
   "source": [
    "print(\"coronavirus - top five courses with similarity score\")\n",
    "for i in range(5):\n",
    "    print(coronavirus_sim_vals_sorted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the results seem reasonable?**\n",
    "\n",
    "The results seem very reasonable. The searching outputs contain Biology related courses. \"Infection biology\" has highest similarity score, and it's very related to infectious disease like coronavirus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
